{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3dbe541-3fb0-4ba2-b340-cc3d8dfd9483",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56959fb-f270-49c1-aa72-2282762b51bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6be095-8c91-435a-b0cd-ad20b27b1b2c",
   "metadata": {},
   "source": [
    "To demonstrate the mechanics of classification, we will work with a dataset that is related to something we all know about, health. However, the methods, techniques, and caveats apply regardless of the classification task. We will be looking at a binary classification problem, which is a classification problems where the thing we are trying to learn to predict, i.e., the target, has two possible options. The following code block loads the data that we will be using. From https://www.kaggle.com/datasets/mathchi/diabetes-data-set:\n",
    "\n",
    "```\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective is to predict based on diagnostic measurements whether a patient has diabetes.\n",
    "\n",
    "Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    " - Pregnancies: Number of times pregnant\n",
    " - Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    " - BloodPressure: Diastolic blood pressure (mm Hg)\n",
    " - SkinThickness: Triceps skin fold thickness (mm)\n",
    " - Insulin: 2-Hour serum insulin (mu U/ml)\n",
    " - BMI: Body mass index (weight in kg/(height in m)^2)\n",
    " - DiabetesPedigreeFunction: Diabetes pedigree function\n",
    " - Age: Age (years)\n",
    " - Outcome: Class variable. 0 represents no diabetes and 1 represents diabetes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80edd51-39bc-4c74-8f71-87b38f6408ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/diabetes.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73397b-231f-4c27-b470-a3ff805022a9",
   "metadata": {},
   "source": [
    "The following code block prints the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0426713-493b-4988-a1c5-5813b03c33d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bcdd3c-d0aa-4542-b625-f46f9cd252bb",
   "metadata": {},
   "source": [
    "The following code block specifies the name of the column that contains the `target` variable that we want to predict. We then use the `.columns` attribute of the `DataFrame` to get a least of `features`. These features will be the things we believe are useful for predicting the value of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390478f0-6c91-42bd-9a89-f801bd434b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = 'Outcome'\n",
    "features = [col for col in data.columns if col != target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7577c76-8a3c-482c-8bfc-1ecf11e26e1e",
   "metadata": {},
   "source": [
    "The following code block plots the percent of instances that are associated with each possible value of the target variable. As you can see, the dataset we are working with has **class imbalance**. This simply means that the percent of instances that are associated with each class is not equal. In our case, the imbalance is significant, but not extreme. However, this is something we always need to consider because from a learning perspective, your model with have more information about one class than another and this can lead to some interesting problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd09927-4854-43cc-afdd-eff9911778e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_proportion = data.groupby(\n",
    "    target\n",
    ").agg(\n",
    "    instances=(features[0], 'count')\n",
    ").reset_index()\n",
    "\n",
    "class_proportion['Percent'] = class_proportion['instances']/class_proportion['instances'].sum()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "class_proportion.plot(\n",
    "    kind='bar',\n",
    "    x='Outcome',\n",
    "    y='Percent',\n",
    "    edgecolor='k',\n",
    "    ax=ax,\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "ax.set_ylabel('Percent of Instances')\n",
    "ax.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46d25a-f1ca-4432-b5b4-e20a3452883b",
   "metadata": {},
   "source": [
    "Another thing that we want to check is the correlation between features. If we include two features that are highly correlated with one another, either in a positive or negative direction, the model can have trouble discerning which feature is contributing to the predictive task. Although we do have some correlations that are in the moderate range, there is nothing particularly troubling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9088b5-0309-4939-b149-8a95a0f30e2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
    "\n",
    "sns.heatmap(\n",
    "    data.corr(),\n",
    "    cmap='coolwarm',\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    linecolor='k',\n",
    "    linewidths=0.1,\n",
    "    cbar=False,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3afac49-c755-426e-9f65-64810c030e31",
   "metadata": {},
   "source": [
    "We will next split the data into two datasets:\n",
    "1. a **training** set that we will use to fit the prediction model, and\n",
    "2. a **testing** set that we will use to test the model.\n",
    "\n",
    "The reason we split the data is to prevent the algorithm we use from simply *learning the data* and *overfitting*. Given the large computational resources and sophisticated models we have available today, this is a necessary step. Today we will be using a *logistic regression* model from the `statsmodels` library, so we need to use `pandas` to construct the datasets. In subsequent classes, we will be using `scikit-learn`, and they have some built in functions available to make this process easier for working with thier models. The following code block uses the `.sample` method to generate a training dataset that contains 75% of the rows of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d5825-8e7c-49ac-ab05-396658b57dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = data.sample(frac=0.75, random_state=42)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529669c4-cf66-436e-b4ca-f90568ef5545",
   "metadata": {},
   "source": [
    "The following code block creates the testing dataset, which contains all of the rows in the original data that are not in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341fc70-3534-4cf8-9989-3a4d39efed98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = data[~data.index.isin(train.index)].copy()\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634853a8-2cd6-42fc-88d9-000d1bbee193",
   "metadata": {},
   "source": [
    "Since we are still using `statsmodels` (like we did for linear regression), we can still use the same approach to fitting the logistic regression model. In particular, we must first define a `formula` that specifies the regression model we want to fit. The following code block shows how we can define a simple formula that specifes we are trying to predict the target as a function of the sum of the feature variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4cdd2c-f869-49cf-a812-2161fd380173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "formula = f'{target} ~ {\" + \".join(features)}'\n",
    "formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996ac8b-5896-45b3-8154-4f53b0ef9f4b",
   "metadata": {},
   "source": [
    "The following code block fits the regression model and prints a summary of the fit. Although the summary looks very similar to what we got for linear regression, a major change is that instead of the coefficients being expressed in the same units as the target, they now represent a log-odds ratio. For those interested in the mathematics, please refer to https://en.wikipedia.org/wiki/Logistic_regression. The key thing to know is that the values represent the effect of the variable of the likelihood of seeing a positive (1) value for the target variable. As the coefficient go up, a one unit increase in the feature is more likely to result in a positive class value. On the other hand, as the coefficient goes down, a one unit increase in the feature is more likely to result in a negative class value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36786f2-7391-4212-87df-9fe56b14a4b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg = smf.logit(formula, data=data).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94987ad4-9889-4973-b0c0-d69fcc74b721",
   "metadata": {},
   "source": [
    "The following code block generates predictions for *the probability* of each instance belonging to the positive class for both the training and testing datasets. What we hope to see is that higher values of this probability are more likely to result in postive values for the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e038618-797a-43da-92d5-03a805d5a6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['positive_probability'] = reg.predict(train)\n",
    "test['positive_probability'] = reg.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a46a156-c03d-4995-8417-4ea05a5342a3",
   "metadata": {},
   "source": [
    "The following code block generates a 95% confidence interval plot for the mean value of the `positive probability` column as the target varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2353e618-1bbd-4bb7-95a6-c30fb3c1902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "sns.pointplot(\n",
    "    data=train,\n",
    "    x=target,\n",
    "    y='positive_probability',\n",
    "    ax=ax,\n",
    "    capsize=0.1,\n",
    ")\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf39b56-5bd1-4b17-b9c3-a68861f61759",
   "metadata": {},
   "source": [
    "The key thing to note is that this model is not giving a prediction, but a probability. Thus, we can set a threshold to convert the probabilities into predictions. Typically, as you might guess, this threshold is set at `0.50`. The following code block uses this threshold to convert the probabilities to predictions and then uses the predictions to compute the accuracy of the model on the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0231653-75f8-44d6-97e1-f5c0a22b2e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = 0.50\n",
    "\n",
    "train_predictions = (train['positive_probability'] >= threshold).astype(int)\n",
    "train_accuracy = (train_predictions == train[target]).astype(int).mean()\n",
    "print(f'{train_accuracy = :.2%}')\n",
    "\n",
    "test_predictions = (test['positive_probability'] >= threshold).astype(int)\n",
    "test_accuracy = (test_predictions == test[target]).astype(int).mean()\n",
    "print(f'{test_accuracy = :.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe7ce9-52b7-418a-9067-db677801f015",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the previous cell, we use accuracy as the metric. Accuracy simply measure the proportion (or percent) of correct predictions. However, let's define more formally. To do so, let:\n",
    "- **TP** denote **True Positives**: cases where the target value is 1 and the prediction is also 1\n",
    "- **TN** denote **True Negatives**: cases where the target value is 0 and the prediction is also 0\n",
    "- **FP** denote **False Positives**: cases where the target value is 0 but the prediction is 1\n",
    "- **FN** denote **False Negatives**: cases where the target value is 1 but the prediction is 0\n",
    "\n",
    "Using this notation, we can define **Accuracy** as:\n",
    "$$\\displaystyle \\frac{TP + TN}{TP + TN + FP + FN}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1711b8-5e85-4bb9-83ca-74bddc5325d6",
   "metadata": {},
   "source": [
    "The following code block runs a simple experiment that looks at the accuracy that we achieve on the two datasets as we vary the threshold used for converting the probabilities to a prediction. A vertical line is drawn at 0.50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdbfc33-abfa-43d0-9a3b-71e2d9d78cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold_experiment = {}\n",
    "\n",
    "for threshold in np.arange(0.01, 1.0, 0.01):\n",
    "\n",
    "    train_predictions = (train['positive_probability'] >= threshold).astype(int)\n",
    "    train_accuracy = (train_predictions == train[target]).astype(int).mean()\n",
    "\n",
    "    test_predictions = (test['positive_probability'] >= threshold).astype(int)\n",
    "    test_accuracy = (test_predictions == test[target]).astype(int).mean()\n",
    "    \n",
    "    threshold_experiment[round(float(threshold), 2)] = {\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy\n",
    "    }\n",
    "    \n",
    "threshold_experiment = pd.DataFrame().from_dict(\n",
    "    threshold_experiment,\n",
    "    orient='index',\n",
    ")\n",
    "\n",
    "threshold_experiment = threshold_experiment.reset_index()\n",
    "threshold_experiment = threshold_experiment.rename(columns={'index': 'threshold'})\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "ax.plot(\n",
    "    threshold_experiment['threshold'],\n",
    "    threshold_experiment['train_accuracy'],\n",
    "    label='Train',\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    threshold_experiment['threshold'],\n",
    "    threshold_experiment['test_accuracy'],\n",
    "    label='Test',\n",
    ")\n",
    "ax.axvline(0.5)\n",
    "ax.set_xlabel('threshold')\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.legend()\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cffba8-967d-4217-9192-aa72b233815b",
   "metadata": {
    "tags": []
   },
   "source": [
    "From an accuracy perspective, the previous experiment shows that 0.5 is a reasonable choice. **However, is it the best choice for this problem?**\n",
    "\n",
    "Whenever you are considering a classification problem, you must consider whether the costs of errors are the same. In this case, a **False Positive** corresponds to a case where we tell a patient they are likely to have diabetes when they don't. On the other hand, a **False Negative** corresponds to a case where we tell a patient they have nothing to worry about when in fact they have a serious disease. \n",
    "\n",
    "Let's see how the threshold of 0.5 does with respect to these error types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa0df2-6c07-4577-9644-66d33cf81168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = 0.50\n",
    "\n",
    "test_predictions = (test['positive_probability'] >= threshold).astype(int)\n",
    "test_TP = ((test_predictions == 1) & (test[target] == 1)).astype(int).sum()\n",
    "print(f'{test_TP = }')\n",
    "\n",
    "test_TN = ((test_predictions == 0) & (test[target] == 0)).astype(int).sum()\n",
    "print(f'{test_TN = }')\n",
    "\n",
    "test_FP = ((test_predictions == 1) & (test[target] == 0)).astype(int).sum()\n",
    "print(f'{test_FP = }')\n",
    "\n",
    "test_FN = ((test_predictions == 0) & (test[target] == 1)).astype(int).sum()\n",
    "print(f'{test_FN = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add7e3c2-f76a-4fc2-a9c3-d14f345e88c3",
   "metadata": {},
   "source": [
    "As you can see, the current threshold favors false negatives. Let's rerun the experiment looking at how chaning the threshold affects the false positive and false negative rates on the testing dataset. Again, the 0.5 threshold is plotted for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f96b47-c914-4a7f-b95b-9bbf0b52c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_experiment = {}\n",
    "\n",
    "for threshold in np.arange(0.01, 1.0, 0.01):\n",
    "\n",
    "    test_predictions = (test['positive_probability'] >= threshold).astype(int)\n",
    "    test_FP = ((test_predictions == 1) & (test[target] == 0)).astype(int).sum()\n",
    "    test_FN = ((test_predictions == 0) & (test[target] == 1)).astype(int).sum()\n",
    "    \n",
    "    threshold_experiment[round(float(threshold), 2)] = {\n",
    "        'FP': test_FP,\n",
    "        'FN': test_FN\n",
    "    }\n",
    "    \n",
    "threshold_experiment = pd.DataFrame().from_dict(\n",
    "    threshold_experiment,\n",
    "    orient='index',\n",
    ")\n",
    "\n",
    "threshold_experiment = threshold_experiment.reset_index()\n",
    "threshold_experiment = threshold_experiment.rename(columns={'index': 'threshold'})\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "ax.plot(\n",
    "    threshold_experiment['threshold'],\n",
    "    threshold_experiment['FP'],\n",
    "    label='FP',\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    threshold_experiment['threshold'],\n",
    "    threshold_experiment['FN'],\n",
    "    label='FN',\n",
    ")\n",
    "ax.axvline(0.5)\n",
    "ax.set_xlabel('threshold')\n",
    "ax.set_ylabel('Count')\n",
    "ax.legend()\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa3870-d108-42f4-ba2c-e8136b8943e4",
   "metadata": {},
   "source": [
    "The previous code block shows that the threshold choice that maximizes accuracy, may not be best if we want to minimize false negatives. This is a very important thing to note as the choice of the best model or best parameters for a model is not necessarily based on the model that if the most accurate, but instead on the model that results in minimal harm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d487cd7-f54d-4231-9625-3083d1f70f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
