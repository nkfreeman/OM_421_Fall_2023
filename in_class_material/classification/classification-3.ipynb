{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3dbe541-3fb0-4ba2-b340-cc3d8dfd9483",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56959fb-f270-49c1-aa72-2282762b51bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372bfe7-fd95-4d1d-bf62-0c9fe69e7797",
   "metadata": {},
   "source": [
    "In the last class, we looked at using the `scikit-learn` library to fit a `DecisionTreeClassifier` model. The following code block repeats the code need to achieve this for the default settings of the `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f157600-5203-4d99-8b70-31067e0e1b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/diabetes.csv')\n",
    "\n",
    "target = 'Outcome'\n",
    "features = [col for col in data.columns if col != target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[features], \n",
    "    data[target], \n",
    "    random_state=12,\n",
    ")\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "training_score = clf.score(X_train, y_train)\n",
    "testing_score = clf.score(X_test, y_test)\n",
    "\n",
    "print(f'{training_score = }')\n",
    "print(f'{testing_score = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2376c-260f-4239-907a-228a98f49607",
   "metadata": {},
   "source": [
    "As we discussed, the default `DecisionTreeClassifier` overfits the data. We saw that we can mitigate this overfitting by changing hyperparameters of the classifier model. This is shown in the following code block that fits another `DecisionTreeClassifier` with the `max_depth` hyperparameter adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ccf9b8-e603-4139-b1ce-3afabe0ce26b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/diabetes.csv')\n",
    "\n",
    "target = 'Outcome'\n",
    "features = [col for col in data.columns if col != target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[features], \n",
    "    data[target], \n",
    "    random_state=12,\n",
    ")\n",
    "\n",
    "clf = DecisionTreeClassifier(\n",
    "    max_depth=3,\n",
    "    random_state=0,\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "training_score = clf.score(X_train, y_train)\n",
    "testing_score = clf.score(X_test, y_test)\n",
    "\n",
    "print(f'{training_score = }')\n",
    "print(f'{testing_score = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c0d23-a68a-483e-a5d9-9c7eebab18cd",
   "metadata": {},
   "source": [
    "The previous code block shows that we can handle overfitting (and other issues) using hyperparameters of the models. However, the question is what are the best hyperparameters and how can we conduct experiments to find them efficiently. One approach is to use the `GridSearchCV` method that is available in `sklearn`. This method allows you to conduct a search over the various hyperparameters available for a model to identify those that best achieve a given objective.\n",
    "\n",
    "The following code block imports the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822a9f3-b27e-4ddf-b1ec-ad934bedfac8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341153c6-d19a-4775-a852-1108be1f6818",
   "metadata": {},
   "source": [
    "The following code block demonstrates its use on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddcebd-69ea-4842-9a61-17aef6775d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = pd.read_csv('data/diabetes.csv')\n",
    "\n",
    "target = 'Outcome'\n",
    "features = [col for col in data.columns if col != target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[features], \n",
    "    data[target], \n",
    "    random_state=12,\n",
    ")\n",
    "\n",
    "base_clf = DecisionTreeClassifier(\n",
    "    random_state=0,\n",
    ")\n",
    "params = {\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'class_weight': ['balanced', None],\n",
    "}\n",
    "GS = GridSearchCV(\n",
    "    base_clf,\n",
    "    param_grid=params,\n",
    ")\n",
    "GS.fit(X_train, y_train)\n",
    "best_clf = GS.best_estimator_\n",
    "\n",
    "training_score = best_clf.score(X_train, y_train)\n",
    "testing_score = best_clf.score(X_test, y_test)\n",
    "\n",
    "print(f'{training_score = }')\n",
    "print(f'{testing_score = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b21323-cccf-4975-8c4d-89ac32649473",
   "metadata": {},
   "source": [
    "As we can see, the `GridSearchCV` function is able to identify a variant of the `DecisionTreeClassifier` that offers better accuracy. It does this by running a cross-validation procedure using all combinations of the hyperparameter values specified in the `params` dictionary. To get the best hyperparameter values it identified, we can use the `best_params_` attribute of the fitted object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82df8a7-8b2d-4c52-8340-d66d0500e751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GS.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf378611-2959-4d4a-9478-f0c9f769f4b5",
   "metadata": {},
   "source": [
    "We can see the detailed results of the cross-validation experiment by accessing the `cv_results_` attribute and using it to create a `pandas` `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a6031-863d-41ad-8406-c19d252ff6b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(GS.cv_results_).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d23d0e-7dbd-44ef-af83-122e7ee7db6a",
   "metadata": {},
   "source": [
    "Let's try to rerun this experiment using a different classifier, namely, a `RandomForestClassifier`. The following code block imports the `RandomForestClassifier` object from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55c0a6-4f0a-41fa-9403-d0b98c6afdab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417a8de2-df10-4136-b979-3b7f8c39b783",
   "metadata": {},
   "source": [
    "The following code block runs the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0078ac-0fc2-4579-9530-81c249f15e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = pd.read_csv('data/diabetes.csv')\n",
    "\n",
    "target = 'Outcome'\n",
    "features = [col for col in data.columns if col != target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[features], \n",
    "    data[target], \n",
    "    random_state=12,\n",
    ")\n",
    "\n",
    "base_clf = RandomForestClassifier(\n",
    "    random_state=0,\n",
    ")\n",
    "params = {\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'class_weight': ['balanced', None],\n",
    "}\n",
    "GS = GridSearchCV(\n",
    "    base_clf,\n",
    "    param_grid=params,\n",
    ")\n",
    "GS.fit(X_train, y_train)\n",
    "best_clf = GS.best_estimator_\n",
    "\n",
    "training_score = best_clf.score(X_train, y_train)\n",
    "testing_score = best_clf.score(X_test, y_test)\n",
    "\n",
    "print(f'{training_score = }')\n",
    "print(f'{testing_score = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388dda1c-5d23-49ee-b1e6-8294baf93e70",
   "metadata": {},
   "source": [
    "The `RandomForestClassifer` is able to do better than the `DecisionTreeClassifer`. However, it took a bit longer. The data we are working with is rather small, but this could be problematic if it were larger. Since the `GridSearchCV` method is running multiple experiments, it turns out that we can run these in parallel (using multiple CPU cores) to achieve better computational performance. This is demonstrated in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08374610-88d4-45e9-94bd-cc9e35519805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = pd.read_csv('data/diabetes.csv')\n",
    "\n",
    "target = 'Outcome'\n",
    "features = [col for col in data.columns if col != target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[features], \n",
    "    data[target], \n",
    "    random_state=12,\n",
    ")\n",
    "\n",
    "base_clf = RandomForestClassifier(\n",
    "    random_state=0,\n",
    ")\n",
    "params = {\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'class_weight': ['balanced', None],\n",
    "}\n",
    "GS = GridSearchCV(\n",
    "    base_clf,\n",
    "    param_grid=params,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "GS.fit(X_train, y_train)\n",
    "best_clf = GS.best_estimator_\n",
    "\n",
    "training_score = best_clf.score(X_train, y_train)\n",
    "testing_score = best_clf.score(X_test, y_test)\n",
    "\n",
    "print(f'{training_score = }')\n",
    "print(f'{testing_score = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f55ef7f-ed71-4c4f-9087-978c9591f1ce",
   "metadata": {},
   "source": [
    "Let's look at the confusion matrix for the best model identified by the procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b7fb49-d7b9-4ee9-b1b6-40ddc4d98d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_clf, \n",
    "    X_test, \n",
    "    y_test,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaae028-ec88-49f3-8764-099c66175a75",
   "metadata": {},
   "source": [
    "If we wanted to see if we can affect the likelihood of certain errors, we can specify different scoring metrics for the `GridSearchCV` parameters. The possible scoring metrics are given at: https://scikit-learn.org/stable/modules/model_evaluation.html. The following code block shows how we can use the `balanced_accuracy` metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0955c3-640f-41a9-af86-791ee0e77e78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = pd.read_csv('data/diabetes.csv')\n",
    "\n",
    "target = 'Outcome'\n",
    "features = [col for col in data.columns if col != target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[features], \n",
    "    data[target], \n",
    "    random_state=12,\n",
    ")\n",
    "\n",
    "base_clf = RandomForestClassifier(\n",
    "    random_state=0,\n",
    ")\n",
    "params = {\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'class_weight': ['balanced', None],\n",
    "}\n",
    "GS = GridSearchCV(\n",
    "    base_clf,\n",
    "    param_grid=params,\n",
    "    scoring='balanced_accuracy',\n",
    "    n_jobs=-1,\n",
    ")\n",
    "GS.fit(X_train, y_train)\n",
    "best_clf = GS.best_estimator_\n",
    "\n",
    "training_score = best_clf.score(X_train, y_train)\n",
    "testing_score = best_clf.score(X_test, y_test)\n",
    "\n",
    "print(f'{training_score = }')\n",
    "print(f'{testing_score = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588499fb-6f0e-4e11-a67d-99553f66ab74",
   "metadata": {},
   "source": [
    "The resulting confusion matrix shows that this change has no effect in this case, but there are situations where the effects can be dramatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36875a05-81c2-407a-98d9-7fe1e4a669b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_clf, \n",
    "    X_test, \n",
    "    y_test,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8084ab-42c0-4af3-9341-b08d31445344",
   "metadata": {},
   "source": [
    "**Aside**: Note that the `RandomForestClassifer` does give us class probabilities, so we could define our own prediction threshold (as we did with the logistic regression model) to see if we can reduce false negatives further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80523a03-63c2-484e-93cf-96d22c462292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc88d2-8b02-4f39-87f1-49b5c12f3e73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
