{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3dbe541-3fb0-4ba2-b340-cc3d8dfd9483",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56959fb-f270-49c1-aa72-2282762b51bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372bfe7-fd95-4d1d-bf62-0c9fe69e7797",
   "metadata": {},
   "source": [
    "In the previous class, we looked at our first classification model, a Logistic Regression model. We used the `statsmodels` library to implement the logistic regression model. As a reminder of our process, the following code block repeats the steps for loading the data, splitting the data into training and testing datasets, fitting the logistic regression model, and then generating predictions based on a threshold of 0.50 for the predicted class probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80edd51-39bc-4c74-8f71-87b38f6408ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "data = pd.read_csv('data/diabetes.csv')\n",
    "\n",
    "# define target and feature variables\n",
    "target = 'Outcome'\n",
    "features = [col for col in data.columns if col != target]\n",
    "\n",
    "# generate training and testing datasets\n",
    "train = data.sample(frac=0.75, random_state=42)\n",
    "test = data[~data.index.isin(train.index)].copy()\n",
    "\n",
    "# define regression formula\n",
    "formula = f'{target} ~ {\" + \".join(features)}'\n",
    "\n",
    "# fit regression\n",
    "reg = smf.logit(formula, data=data).fit()\n",
    "\n",
    "# generate predictions (positive class probabilities for this model)\n",
    "train['positive_probability'] = reg.predict(train)\n",
    "test['positive_probability'] = reg.predict(test)\n",
    "\n",
    "# define positive class threshold\n",
    "threshold = 0.50\n",
    "\n",
    "# get training dataset accuracy\n",
    "train_predictions = (train['positive_probability'] >= threshold).astype(int)\n",
    "train_accuracy = (train_predictions == train[target]).astype(int).mean()\n",
    "print(f'{train_accuracy = :.2%}')\n",
    "\n",
    "# get testing dataset accuracy\n",
    "test_predictions = (test['positive_probability'] >= threshold).astype(int)\n",
    "test_accuracy = (test_predictions == test[target]).astype(int).mean()\n",
    "print(f'{test_accuracy = :.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546e68f-b627-42e6-a467-3f802b4ebedd",
   "metadata": {},
   "source": [
    "In this notebook, we will explore another library that is very popular for classification tasks, namely, *scikit-learn*. This library is massive (as we will see). Thus, the standard practice is to import modules or classes defined in the library as needed. A nice thing about scikit-learn, which is imported as `sklearn`, is that the developers have done an excellent job in creating a simple and uniform way to work with classification problems. We will demonstrate this using a simple classification method, a Decision Tree. Decision Trees work by learning a tree structure for *splitting* the feature space to generate predictions. If you are interested in additional information, see: https://en.wikipedia.org/wiki/Decision_tree.\n",
    "\n",
    "The following code block imports two objects from scikit-learn, one that is used to simplify the process of creating training and testing datasets for use with scikit-learn and another for fitting a Decision Tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d487cd7-f54d-4231-9625-3083d1f70f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b5a732-9063-418c-a94e-83334e399d15",
   "metadata": {},
   "source": [
    "The following code block shows how we can use the `train_test_split` function to generate training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44f3c6-dbfe-46f3-aed4-002d1c1e9653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4516361-4bf0-4b34-8848-33a2f25bfe71",
   "metadata": {},
   "source": [
    "The following code block prints the dimensions of the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f7ea9-3708-42c5-aef4-66bff2eee7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{X_train.shape = }')\n",
    "print(f'{X_test.shape = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adaeeed-a32e-4685-8779-9c33cc3d5f59",
   "metadata": {},
   "source": [
    "The following code block fits a simple Decision Tree classifier using the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c94956-bb32-4c5b-ab2c-e14d7de8b575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687c14fd-b218-473e-9738-40469aab546b",
   "metadata": {},
   "source": [
    "The follwoing code block shows how we can generate predictions using the fitted classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b14dd-e914-4e32-8fa4-52e7b69679cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05046ee8-481a-4c28-ba88-f555937046bc",
   "metadata": {},
   "source": [
    "The following code block shows how we can generate probabilities for the two possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55076e-35c0-4cd1-bd22-d961de71f827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deddfac-eb0e-4737-a5e2-d41bf19b4bd9",
   "metadata": {},
   "source": [
    "The fitted classifier has a convenient `score` method that allows us to assess accuracy. The following code block prints the score for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8db210-2ea0-4672-9870-04299d8072e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8f1c5-c5c5-4ee2-93e9-b012098811ec",
   "metadata": {},
   "source": [
    "The following code block prints the score for the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad554287-af23-4a56-9a1f-a0b1627c9f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca613368-ead0-44a8-8a72-f323f7954f8c",
   "metadata": {},
   "source": [
    "A nice thing about decision trees is that we can see exactly how the classifier is making the predictions. To do this, we use the `export_tree` method, which is imported in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6729d9-465e-467f-b160-c126771d48e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0622c945-1728-4a7e-acbe-54c9aa5dea0d",
   "metadata": {},
   "source": [
    "The following code block uses the `export_tree` function to print the fitted Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12bb591-ff2a-4bcf-b8b5-e468a1ff21b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(export_text(clf, feature_names=features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ba703-dc29-4f34-86da-75f5dfd4ad0b",
   "metadata": {},
   "source": [
    "The previously fit Decision Tree is clearly overfitting the data. We can use *hyperparameters* of the Decision Tree classifier in order to change the behavior of the model and (hopefully) avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220329bf-2908-45cf-a5a0-e2037e3befc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    max_depth=3,\n",
    ")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3f160f-66cb-4ac6-b828-11911396dbc4",
   "metadata": {},
   "source": [
    "The following code block prints the updated training dataset score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd442e8a-23ac-40d7-a410-270397a33148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2749e379-833f-447a-8cbd-c2d6468926e3",
   "metadata": {},
   "source": [
    "The following code block prints the updated testing dataset score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17032e1-2f03-4bfe-9350-7a000fa39c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b58244-6edb-48a6-ab48-6b2c510df429",
   "metadata": {},
   "source": [
    "As we saw earlier, we do not get back class probabilities for a Decision Tree. To see the errors that are made with the current classifier, we can plot a *confusion matrix*. The following code block imports a function that simplifies this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bac9c9-5c8f-4ac0-8e1c-aec64221d0ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed428785-934a-46d6-85c9-0b6f94068cd3",
   "metadata": {},
   "source": [
    "The following code block uses the `ConfusionMatrixDisplay` function, along with the fitted classifier and the testing data, to generate a plot of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea774b5e-aa09-4370-900e-3f4d2bf96116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    clf, \n",
    "    X_test, \n",
    "    y_test,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6fa207-cdb8-40ef-8dc3-2366cefbb2c4",
   "metadata": {},
   "source": [
    "Earlier I mentioned that *scikit-learn* is designed to make the use of different classifier easy. In order to do so, all we have to do is swap out the classifier that is used. As an example, the following code block uses a *K Nearest Neighbors* classifier instead of a Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ffc96-3940-4716-835a-b7210850ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f'{clf.score(X_train, y_train) = :.2f}')\n",
    "print(f'{clf.score(X_test, y_test) = :.2f}')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    clf, \n",
    "    X_test, \n",
    "    y_test,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9036518-8b08-48a1-9c41-27751b7bc132",
   "metadata": {},
   "source": [
    "## Homework 7\n",
    "\n",
    "Fit an AdaBoost Classifier for the dataset (see https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html). Experiment with some of the hyperparameters manually to see if you can improve the fit on the testing dataset. Also, generate a confusion matrix for the best variant of the model that you find and comment on the classifier's performance with respect to false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e6b28-1909-4630-acd3-667123e70823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
